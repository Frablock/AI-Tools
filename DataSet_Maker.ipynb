{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "dKqU8OYGoX-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Image captioning\n",
        "\"\"\"\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "max_length = 100\n",
        "num_beams = 4\n",
        "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "def predict(image):\n",
        "  images=[image]\n",
        "  pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
        "  pixel_values = pixel_values.to(device)\n",
        "\n",
        "  output_ids = model.generate(pixel_values, **gen_kwargs)\n",
        "\n",
        "  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "  preds = [pred.strip() for pred in preds]\n",
        "  return preds"
      ],
      "metadata": {
        "id": "HduYNLOm9gdA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USM5z8g5lrp5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os \n",
        "\n",
        "ty=input(\"Please, enter the name of your LoRa : \").replace(\" \",\"_\")\n",
        "\n",
        "os.makedirs(\"/content/drive/MyDrive/Loras/\"+ty+\"/dataset/\", exist_ok=True)\n",
        "\n",
        "for page in range(0,10):\n",
        "  search_url=\"https://librex.pufe.org/api.php?q=\"+ty+\"&t=1&p=\"+str(page*10)#we are using the LibreX API to search for pictures (You can choose another instance from https://github.com/hnhx/librex/)\n",
        "  print(search_url)\n",
        "  results=requests.get(search_url)\n",
        "\n",
        "  for n,img in enumerate(results.json()):\n",
        "    img_url=img[\"thumbnail\"]\n",
        "    image=Image.open(requests.get(img_url, stream=True).raw)\n",
        "    raw_image = image.convert('RGB')\n",
        "\n",
        "    print(img_url)\n",
        "    image.save(\"/content/drive/MyDrive/Loras/\"+ty+\"/dataset/\"+str(n+49*page)+\".png\",\"PNG\")\n",
        "\n",
        "    file = open(\"/content/drive/MyDrive/Loras/\"+ty+\"/dataset/\"+str(n+49*page)+\".txt\", \"w\")\n",
        "    r=predict(image)#image captioning\n",
        "    print(r)\n",
        "    file.write(ty.replace(\"_\",\" \")+\", \"+r[0])#[\"generated_text\"])\n",
        "    file.close()\n"
      ]
    }
  ]
}